{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "pipeline_dir = os.path.join(os.getcwd(), '..')\n",
    "if pipeline_dir not in sys.path:\n",
    "    sys.path.append(pipeline_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, events, resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "## Creating a Pipeline\n",
    "Create a new project with the `pipeline-create <new-directory>` command.  You can create a pipeline by inheriting from the `pipeline.Pipeline` base class (see `<new-directory>/handler.py`).  A pipeline contains functions and resources  A pipeline must contain at least one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestPipeline(Pipeline):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    @events.invoke\n",
    "    def lambda_func(self, event, context):\n",
    "        print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex pipelines will contain functions and resources.  The following pipeline will send a message to an SQS queue when a file is uploaded to a S3 Bucket.  The messages in the queue will then be processed by a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBucket(resources.S3Bucket):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class MyQueue(resources.SQSQueue):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "bucket = MyBucket()\n",
    "queue = MyQueue()\n",
    "\n",
    "class NotSoSimplePipeline(Pipeline):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(resources=[bucket, queue])\n",
    "    \n",
    "    @events.bucket_notification(bucket=bucket, event_type=\"s3:ObjectCreated:Put\", destination=queue)\n",
    "    def bucket_kickoff(self, event, context):\n",
    "        print(f\"A file was uploaded to s3://{event['bucket']}/{event['key']}\")\n",
    "\n",
    "pipeline = NotSoSimplePipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Pipeline\n",
    "When your pipeline is ready to deploy, add a deployment function to your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy():\n",
    "    pipeline.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `pipeline-deploy <pipeline-directory>` command to deploy the pipeline.  This will generate a `serverless.yml` used by Serverless Framework to deploy your pipeline and an `outputs.yml` containing the outputs of the deployed CloudFormation stack.  You can use the `--dry-run` flag to only generate `serverless.yml` without deploying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Functionality\n",
    "The pipeline object provides an object-oriented interface for interacting with your resources and functions.\n",
    "#### Functions\n",
    "Access a function using the dictionary interface of the `functions` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pipeline.functions.Function_BUCKET_NOTIFICATION'>\n"
     ]
    }
   ],
   "source": [
    "my_func = pipeline.functions['bucket_kickoff']\n",
    "print(type(my_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions have a trigger and a CloudFormation template describing the trigger.  In this case our trigger is a `bucket_notification` which sends a message to `MyQueue`.  The CloudFormation template describes the event trigger for our lambda function, which is `MyQueue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger: bucket_notification\n",
      "CloudFormation Template: {'events': [{'sqs': {'arn': 'arn:aws:sqs:us-east-1:725820063953:MyQueue'}}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Trigger: {}\".format(my_func.trigger))\n",
    "print(\"CloudFormation Template: {}\".format(my_func.template()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your pipeline is deployed to AWS, you can invoke your function client-side using the `invoke` method.  The specific input is different from event to event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed\n"
     ]
    }
   ],
   "source": [
    "# This will error since pipeline is not deployed\n",
    "try:\n",
    "    my_func.invoke('testing')\n",
    "except:\n",
    "    print(\"Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "Access a resource using the dictionary interface of the `resources` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.MyBucket'>\n"
     ]
    }
   ],
   "source": [
    "my_resource = pipeline.resources['MyBucket']\n",
    "print(type(my_resource))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to functions, resources contain a CloudFormation event describing the resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Type': 'AWS::S3::Bucket', 'Properties': {'BucketName': 'mybucket'}}\n"
     ]
    }
   ],
   "source": [
    "print(my_resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources provide helper methods for interacing with the resource.  In this case `read_file`, `download_image`, `upload_file`, and `upload_image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'arn', 'build_resource', 'clear', 'copy', 'download_image', 'fromkeys', 'get', 'items', 'keys', 'name', 'pop', 'popitem', 'read_file', 'resource', 'setdefault', 'update', 'upload_file', 'upload_image', 'values']\n"
     ]
    }
   ],
   "source": [
    "print(dir(my_resource))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotchas\n",
    "Our pipeline exposes two ways of invoking functions.  The first is invoking the function through its function object.  The second is invoking the function through its associated resource object.  In our above pipeline, for example, we could call `pipeline.functions['bucket_kickoff'].invoke`.  We could also call `pipeline.resources['MyBucket'].upload_file`.  Why does the library expose multiple ways of invoking functions? \n",
    "\n",
    "The short answer is that different AWS events are invoked differently.  HTTP events, for example, require sending a message to an endpoint generated post-deployment.  This means that we don't have access to the endpoint at runtime and can't trigger our function.  This is not problematic with SNS events, for example, as the SNS Topic's ARN can be dynamically generated from other information available at runtime.  It is therefore neccessary to have two different ways of invoking functions.\n",
    "\n",
    "Invoking through the function object requires an `output.yml` file which is automatically generated post-deployment.  Because it is generated post-deployment, it is not included in the lambda deployment package.  This means that calling `function.invoke` inside a lambda function is prone to fail for certain event types.  Invoking through the resource object will never fail inside lambda functions due to this reason.  As such, `function.invoke` is reccomended for client-side use while `resource.invoke` is reccomended for server-side use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution Environment\n",
    "You can change the region, runtime, and stage of your pipeline using the appropriate setters of the execution attribute\n",
    "#### AWS Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline(Pipeline):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.execution.runtime = \"python2.7\"\n",
    "        self.execution.region = \"us-west-2\"\n",
    "        self.execution.stage = \"prod\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IAM Role\n",
    "As of now, the library automatically generates the pipeline's IAM role based on the declared resources.  By default, the role is assigned `*` permission to each resource.  Future versions will restrict permissions to only the actions in use by the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "Deployment uses the Serverless Framework plugin `serverless-python-requirements` to install your pipeline dependencies from requirements.txt.  Alternatively, you can specify a lambda deployment zipfile in your pipeline's deployment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy():\n",
    "    pipeline.deploy(package='path/to/deployment.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Cognition-pipeline integrates with unittests through its object oriented framework of functions and resources.  In general, there are two types of functions to test.  The first type have direct returns.  An example is a lambda function which takes an input, does something, and returns an output directly to the client.  This type is the easiest to test, as we can directly invoke the function inside a test case and return an output for testing.\n",
    "\n",
    "The second type does not have a direct return.  An example is a lambda function which listens to a SNS topic, does something to the message, and relays it to a DynamoDB table (or other resource).  In this situation there is no direct output returned to the client upon invocation.  This leaves us two options for testing.  The first is monitoring the affected resources to confirm our function is performing the right behavior.  The second is to create a new SQS queue which we can use to redirect messages clientside.  Check out the test cases `cognition-pipeline/tests` for examples of both of these techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
